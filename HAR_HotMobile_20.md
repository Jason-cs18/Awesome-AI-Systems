# Improving Resource Efficiency of Deep Activity Recognition via Redundancy Reduction. In HotMobile'20.
## Summary
Although many researchers found redundancy in deep neural networks and seek to utilize deep compression techniques for deploying deep human activity recognition (HAR) models on resource-constraint devices, they ignore the redundancy of consecutive predictions and long overlapping slide window. The current deep HAR models preferred to use long overlapping slide window because this could bring broader temporal context for classification. However, long overlapping slide windows also bring repeated computations and delayed predictions (their predictions are only finished at the end of the slide window). As presented in Figure-1, this work use short non-overlapping slide windows to replace long overlapping windows. ![alt text](https://www.dropbox.com/s/aja240gojf8a6lh/HAR_1.png?dl=0) Due to the choose of long overlapping slide windows, the deep HAR models use consecutive predictions for same activity. Intuitively, the same activity maintains similar patterns in conseutive videos and changes in activities occur with low frequency. Thus, authors design a shallow neural network with low-complexity as the secondary network to replace the complicated HAR models in consecutive predictions for same activity. In sum, there are three contributions in their work: 1) short non-overlapping slide window; 2) a shallow model for consecutive predictions; 3) experiments on two public datasets ([PAMAP2](https://ieeexplore.ieee.org/document/6246152/) and [Opportunity](https://www.sciencedirect.com/science/article/abs/pii/S0167865512004205)).
## Key insight
Successive predictions for same activity and long overlapping slide window bring large unnecessary computations in the current deep HAR models.
## Evaluation
They used CNN-LSTM network as a baseline and compared this with three variants of their method: CNN-LSTM SN, CNN-LSTM SNR and InnHAR. Their metrics are include F1-Score (F1), Number of parameters (NPA), Number of predictions (NPR), Total computational effort (TCE) and Prediction delay of new activties (PDNA). The Table-1 show their experimental results.
